\chapter{Lema de Simons y Teorema de la Alternativa de Gordan}
	\thispagestyle{empty}
	\newcommand{\ttt}{\textbf{\emph{t}}}
	\newcommand{\sss}{\textbf{\emph{s}}}
	
	\paragraph{}Este capítulo se centra en lema de Simons, el cual será de gran ayuda para demostrar el Teorema de la Alternativa de Gordan.

	\paragraph{}Antes de comenzar, necesitamos hacer la siguiente definición.
	
	\begin{definicion}
		Dado $ N \in \NN $ con $ N > 1 $ llamamos símplex unitario de $ \RR^N $ al conjunto convexo y compacto de $ \RR^N $ dado por:
		\begin{equation*}
			\Delta_N := \left\lbrace \ttt \in \RR^N: \sum_{i=1}^{N}{t_i} = 1 \hspace{0.5em} , \hspace{0.5em} t_1,...,t_N \geq 0 \right\rbrace .
		\end{equation*}
	\end{definicion}

	Por ejemplo, si $ N = 2 $, tenemos que $ \Delta_2 = \mathrm{co} \{(1,0),(0,1)\}$, es decir es la envolvente convexa de los vectores de la base usual de $ \RR^2 $. De hecho en general, $ \Delta_N = co\{e_1,...e_N\} $. \\
	
	Antes de continuar veamos que efectivamente $ \Delta_N $ es convexo y compacto:
	
	\begin{itemize}
		\item  Convexo: tenemos que comprobar que dados $ \ttt, \sss \in \Delta_N  \Longrightarrow \lambda\ttt + (1-\lambda)\sss \in \Delta_N, \lambda \in \lbrack 0,1 \rbrack$. En efecto, las coordenadas de $ \lambda\ttt + (1-\lambda)\sss \in \RR^N $ verifican:
		\begin{itemize}
			\item [i) ] $ \lambda t_i + (1-\lambda)s_i \geq 0 $ para todo $ i = 1,..., N $ ya que $ t_i, s_i \geq 0 $.
			\item [ii) ] $ \sum_{i=1}^{N} (\lambda t_i + (1-\lambda)s_i) =   \lambda \sum_{i=1}^{N} t_i + (1-\lambda)\sum_{i=1}^{N} s_i = \lambda + (1 - \lambda) = 1$ ya que $ \sum_{i=1}^{N} t_i = \sum_{i=1}^{N} s_i = 1  $.
		\end{itemize}
	
		Por lo tanto, $ \lambda\ttt + (1-\lambda)\sss \in \Delta_N \text{ para todo }\lambda \in \lbrack 0,1 \rbrack $.
		
		\item Compacto: al encontrarnos en $ \RR^N $ y aplicando el conocido Teorema de Heine-Borel basta ver que $ \Delta_N $ es cerrado y acotado. Claramente es acotado por lo que nos centraremos en la de cerrado. Sea $ \lbrace \ttt_n \rbrace_{ n \in \NN} $ una sucesión de $ \Delta_N $ y sea $ \ttt_0 \in \RR^N $ tal que $ \lbrace \ttt_n \rbrace_{ n \in \NN} \longrightarrow \ttt_0 $. Tenemos que comprobar que $ \ttt_0 \in \Delta_N $. 
		\begin{itemize}
		\item[i) ] Como todas las coordenadas de $ \ttt_n $ son no negativas podemos asegurar que las de $ \ttt_0 $ también lo son.
		\item[ii) ] la función $ f: \RR^N \longrightarrow \RR $ definida como $ f(\ttt) =  \sum_{i=1}^{N} t_i $ es continua. Claramente $ f(\ttt) = 1 $ para todo $ \ttt \in \Delta_N $ y por ello $ \lbrace f(\ttt_n) \rbrace_{ n \in \NN} \longrightarrow 1$. Por continuidad de $ f $ y unicidad de límite tenemos que $ f(\ttt_0) = 1 $ pero eso implica que la suma de sus componentes vale 1
		\end{itemize} 
	
		Así, hemos demostrado que $ \ttt_0 \in \Delta_N $ y por lo tanto $ \Delta_N $ es compacto. 
	\end{itemize}

	\paragraph{} Enunciamos este lema previo que usaremos posteriormente el la demostración del de Simons. 
	\begin{lemaBox}\label{lema2.1}
		Sea $ N \in \NN $, con $ N \geq 1 $ y $ L: \RR^N \longrightarrow \RR $ definida por $ S(x):=\max\{x_1,...,x_N\}$. Entonces, $ S $ es sublineal. Además, si $ L:\RR^N \longrightarrow \RR $ es un funcional lineal tal que $ L \leq S $ entonces $ L $ es de la forma $ L(x) = t_1 x_1 + ... + t_N x_N $ con $ (t_1,...,t_N) \in \Delta_N$. De hecho, el recíproco también es cierto, es decir, si $ L =  (t_1,...,t_N) \in \Delta_N $ entonces $ L \leq S $.
		
		%%%%%% NOTA %%%%%%%%
		%% Las aplicaciones lineales coninciden con el dual del espacio. Es decir, podemos identificar la aplicación con sus coeficientes obeniendo de ese modo un punto del espacio.
		
	\end{lemaBox}
	\begin{proof}
		Claramente, $ S $ es positivamente homogénea. También es subaditiva ya que dados $ x,y \in \vecSpace $ : 
		\begin{equation*}
		\begin{split}
		S(x+y) &= \max \{x_1 + y_1, ..., x_N +y_N \}\\ 
		&\leq \max\{x_1, ..., x_N\} + \max \{y_1, ...,y_N\} = S(x) + S(y)
		\end{split}
		\end{equation*}
		
		Por ello, $ S $ es sublineal. Para terminar veamos que 
		\begin{equation*}
			\left\lbrace L:\RR^N \longrightarrow \RR : L \text{ lineal y }L\leq S \right\rbrace = \Delta_N
		\end{equation*}
		
		\begin{itemize}
			\item[$ \supseteq $ )] Sea $ \mathbf{t} \in \Delta_N$ definimos $ L:\RR^N \longrightarrow \RR $ como $ L(x):= \langle \mathbf{t},x \rangle $. Es evidente que $ L $ es lineal al serlo el producto escalar. Dado $ x \in \RR $:
			\begin{equation*}
				L(x) = \sum_{i=1}^{N}{t_i x_i} \leq \sum_{i=1}^{N}{t_i S(x)} = S(x)\sum_{i=1}^{N}{t_i} = S(x)
			\end{equation*}
			donde la primera desigualdad se debe a que $ x_i \leq S(x)$ para todo $ x_i$ con $ i=1,...,N $ y a que $ t_i \geq 0 $ ya que $ \mathbf{t} \in \Delta_N$. Esto también justifica la que última igualdad ya que $ \sum_{i=1}^{N}{t_i} = 1 $.
			
			\item[$ \subseteq $ )] Sea $ L =  (t_1,...,t_N) \in \RR^N $ lineal tal que $ \forall x \in \RR^N $ cumple $ \sum_{i=1}^{N}{t_i x_i} \leq \max\{x_1,...,x_N\}$. Así, si tomamos $ e_i \in \RR^N $ donde $ e_i $ representa el i-ésimo elemento de la base usual de $ \RR^N $ con $ i=1,...,N $. Entonces:   		
			\begin{equation*}
				L(-e_i) = -t_i \leq 0 \Longrightarrow t_i \geq 0 \quad \forall i=1,...,N
			\end{equation*}
			Si ahora llamamos $ e = \sum_{i=1}^{N}{e_i} $. Obtenemos:
			\begin{equation*}
				\begin{rcases*}
				L(e) = \sum_{i=1}^{N}{t_i} \leq \max \{1, ...,1\} = 1 \\
				L(-e) = -\sum_{i=1}^{N}{t_i} \leq \max \{-1, ...,-1\} = -1
				\end{rcases*} \Longrightarrow \sum_{i=1}^{N}{t_i} = 1
			\end{equation*}
			Concluimos entonces que $ L =  (t_1,...,t_N) \in \Delta_N $.
		\end{itemize}
		
	\end{proof}

	\paragraph{} Enunciamos ahora lema de Simons\cite{Simons2008} en que destacamos la ausencia de hipótesis topológicas, lo que será importante posteriormente.
	
	\begin{lemaBox}[Lema de Simons]\label{Simons}
		Sea $ C $ un subconjunto no vacío y convexo de un espacio vectorial. Dadas $ f_1, ..., f_N $ ($ N \geq 1 $)  funciones sobre $ C $ reales y convexas, entones existen $ (t_1, ..., t_N) \in \Delta_N $ que cumplen
		\[
		\inf_C\left[ \max \{f_1, ..., f_N\}\right] = \inf_C \left[ t_1 f_1+ ...+t_N f_N \right]
		\] 
	\end{lemaBox}
	\begin{proof}
		Sea  \vecSpace = $ \RR^N $ con $ N \in \NN $. Definimos $S:\vecSpace \longrightarrow P $ como \[ S(x_1, ..., x_N) := \max \{x_1, ..., x_N\} \]. Por el lema \ref{lema2.1} $ S $ es sublineal. Tomamos el subconjunto:
		\[ 
		D = \{ (x_1, ..., x_N)\in V: \exists c \in C \quad \text{ tal que } \quad \forall i = 1,...N,\quad f_i(c) \leq x_i \}
		\]
		
		$ D $ es un subconjunto convexo de \vecSpace. Sean $ x, y \in D $, por ello, existen $ c_x, c_y \in C $ tales que $ f_i (c_x) \leq x_i  $ y $ f_i (c_y) \leq y_i \quad \forall i=1,...,N $. Dado $ \lambda \in [0,1] $, llamamos $ c := (1-\lambda)c_x + \lambda c_y $ que pertenece a $ C $ por ser este convexo. Veamos que $ c $ es el elemento necesario de $ C $ para que cualquier combinación convexa de $ x $ e $ y $ esté en $ D $. Así, para todo $ i =1,...,N  $:	
		\[
		f_i(c) = f_i((1-\lambda)c_x + \lambda c_y) \leq (1-\lambda)f(c_x) + \lambda f(c_y) \leq (1-\lambda)x_i + \lambda y_i 
		\]
		donde la primera desigualdad se debe a que las $ f_i $ son convexas y la segunda a que $ x,y \in D $. Por ello, $ (1-\lambda)x_i + \lambda y_i \in D , \quad \forall \lambda \in [0,1] $ por lo que $ D $ es convexo. Aplicando el Teorema de Mazur-Orlizc, existe $ L $ funcinal lineal sobre \vecSpace tal que $ L \leq S $ e $ \inf_D L = \inf_D S $. \\
		
		Nuevamente, por el lema \ref{lema2.1} tenemos que $ L = (t_1,...,t_N) \in \Delta_N$. \\
		
		Finalmente:
		\[
		\inf_D L = \inf_{c\in C} \left[ t_1 f(c) + ..+ t_N f(c) \right] = \inf_{C} \left[ t_1 f + ..+ t_N f \right]
		\]
		y
		\[
		\inf_D S = \inf_{c\in C} \left[ \max \{f_1(c), ..., f_N(c)\} \right] = \inf_C\left[ \max \{f_1, ..., f_N\}\right] 
		\]
		por lo que 
		\[ \inf_{C} \left[ t_1 f + ..+ t_N f \right] = \inf_C\left[ \max \{f_1, ..., f_N\}\right]  \] 
	\end{proof}

	\paragraph{}Enunciamos ahora el Teorema de la alternativa de Gordan en su versión convexa.
	
	\begin{teoremaBox}[Teorema de la Alternativa de Gordan-versión convexa]\label{Gordan}
		Sea $ C $ un subconjunto convexo de un espacio vectorial y sean $ f_1,...,f_N : C \longrightarrow \RR $ funciones convexas ($ N \geq 1 $). Entonces una, y solo una, de la siguientes afirmaciones se cumple:
		\begin{itemize}
			\item[i)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{C}  \sum_{i=1}^{N}{t_i f_i}$.
			\item[ii)] $ \exists c \in C $ que cumple $ \max \lbrace f_1(c), ..., f_N(c) \rbrace < 0 $.
		\end{itemize}
	\end{teoremaBox}
	\begin{proof}
		Si aplicamos el lema de Simons, lema \ref{Simons} a las funciones $ f_1,...,f_N $ obtenemos:
		
		\begin{equation*}
			\exists t \in \Delta_N \text{ : } \inf_{ C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] = \inf_{C} \sum_{i=1}^{N}t_i f_i
		\end{equation*}
		
		Supongamos en primer lugar que $ \alpha := \inf_{C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] \in \RR $. Planteamos la siguiente alternativa, cuyos casos son excluyentes:
		\begin{itemize}
			\item[a)] $ 0 \leq \alpha $: implica $ i) $ ya que $ \alpha = \inf_{C} \sum_{i=1}^{N}t_i f_i $ 
			\item[b)] $ \alpha < 0 $: este caso, por su parte, implica $ ii) $ ya que:
			
			\begin{equation*}
				\alpha = \inf_{C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] < 0 \Longleftrightarrow \exists c \in C  : \max \lbrace f_1(c), ..., f_N(c) \rbrace < 0 
			\end{equation*}  
		\end{itemize}
	
		Para finalizar, veamos cuando $ \alpha =-\infty $. En este caso, estamos en la misma situación que en b) por lo que solo se puede dar $ ii) $.
	\end{proof}

	\paragraph{} Destacamos las siguientes observaciones:
	
	\begin{observacion}
		Esta versión convexa del teorema implica la versión clásica del mismo.
	\end{observacion}

	Dados $ \{x_1,...x_N\}$ con $ x_i \in \RR^M , (M \geq 1 )$ $i=1,...,N$, la versión clásica del teorema nos aporta las siguientes alternativas:

	\begin{itemize}
		\item[i*)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 = \sum_{i=1}^{N}{t_i x_i}$.
		\item[ii*)] $ \exists y \in \RR^M $ tal que cumple $ \max_{i=1,...,N} \langle y, x_i \rangle < 0 $.
	\end{itemize}

	Para ello, basta aplicar la versión convexa del teorema a $ C := \RR^M $ y a las funciones $ f_1,...,f_N : C \longrightarrow \RR $ definidas por $ f_i(c):=\langle c,x_i \rangle , \forall i=1,...,N  $. Notar que las funciones $ f_1,...,f_N $ son lineales por lo que en particular son convexas. En este caso, la alternativa $ ii) $ implica $ ii*) $ ya que:
	
	\begin{equation*}
		\exists c \in C = \RR^M \text{ : } \max_{i=1,...,N} {\langle c,x_i \rangle}  =  \max_{i=1,...,N}f_i (c) < 0 
	\end{equation*}
	
	Por su parte, la alternativa $ i) $ nos da:
	\begin{equation*}
		\exists \mathbf{t} \in \Delta_N \text{ : } 0 \leq \inf_{c \in \RR^M}  \sum_{i=1}^{N}{t_i f_i(c) } = \inf_{c \in \RR^M} \sum_{i=1}^{N}{t_i\langle c,x_i \rangle} = \inf_{c \in \RR^M} \langle c, \sum_{i=1}^{N}{t_i x_i} \rangle 
	\end{equation*}
	
	Hemos obtenido por ello que : $0  \leq \inf_{c \in \RR^M} \langle c, \sum_{i=1}^{N}{t_i x_i} \rangle  $ lo que nos lleva a $ 0 \leq \langle c, \sum_{i=1}^{N}{t_i x_i} \rangle  $ para todo $ c \in \RR^M $. Usando la linealidad por la izquierda del producto escalar:
	\[
	0 \leq \langle -c, \sum_{i=1}^{N}{t_i x_i} \rangle \Longleftrightarrow 	0 \leq -\langle c, \sum_{i=1}^{N}{t_i x_i} \rangle 
	\Longleftrightarrow  \langle c, \sum_{i=1}^{N}{t_i x_i}\rangle \leq 0, \quad \forall c \in \RR^M
	\]
	
	Juntando ambas desigualdades obtenemos que $ 0 =  \langle c, \sum_{i=1}^{N}{t_i x_i}\rangle, \quad \forall c \in \RR^M $. Como la igualdad anterior se cumple para todo elemento de $ \RR^M $ entonces podemos deducir que $ \sum_{i=1}^{N}{t_i x_i} = 0 $	ya que $ \sum_{i=1}^{N}{t_i x_i} \in (\RR^M)^{\perp} = \{0\} $. Así pues, tenemos que
	\begin{equation*}
		\text{Se cumple }i) \Longleftrightarrow \exists t \in \Delta_N \text{ : }  0 = \sum_{i=1}^{N}{t_i x_i}
	\end{equation*} 	
	
	Es claro entonces que obtenemos $ i*) $.
	
	\begin{observacion}
		El lema de Simons(lema \ref{Simons}) y el Teorema de la Alternativa de Gordan (teorema \ref{Gordan}) son equivalentes.
	\end{observacion}

	\paragraph{} Ya hemos visto que el Lema de Simons implica el Teorema de la Alternativa de Gordan. Veamos que el recíproco también es cierto.  \\
	
	Llamamos $ \alpha := \inf_{C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] $. Si $ \alpha = -\infty $. Por el lema \ref{lema2.1} sabemos que $ \forall \ttt \in \Delta_N $ se cumple que $ \sum_{i=1}^{N} t_i f_i(c) \leq \max \lbrace f_1 (c), ... , f_N (c) \rbrace$ para todo $ c \in C$. Tomando ínfimos en C:
	\[
	\inf_C\left[ \sum_{i=1}^{N} t_i f_i \right] \leq \inf_C \left[ \max \lbrace f_1, ..., f_N \rbrace \right] = -\infty \Longrightarrow \inf_C\left[ \sum_{i=1}^{N} t_i f_i \right] = -\infty 
	\]
	
	y por ello $ \forall \ttt \in \Delta_N $ (en particular para uno) se cumple que
	
	\[
	\inf_C\left[ \sum_{i=1}^{N} t_i f_i \right] = \inf_C \left[ \max \lbrace f_1, ..., f_N \rbrace \right] \]

	Supongamos ahora que $ \alpha \in \RR $. Sean las funciones $ q_1, ..., g_N: C \longrightarrow \RR $ definidas como $ g_i = f_i - \alpha $ con $ i=1,...,N$. Veamos que las funciones $ g_1, ..., g_N $ son convexas como consecuencia de que $ f_1, ..., f_N $ lo son. Sean $ c_1,c_2 \in C $ y $ \lambda \in \left[0,1\right] $:
	\begin{equation*}
	\begin{split}
	g_i(\lambda c_1 + (1-\lambda) c_2) &= f_i(\lambda c_1 + (1-\lambda) c_2) - \alpha \\
	&\leq \lambda f_i(c_1) + (1-\lambda)f_i(c_2) - \alpha \\
	&= \lambda f_i(c_1) + (1-\lambda)f_i(c_2) - \lambda \alpha + (1-\lambda)\alpha \\
	&= \lambda( f_i(c_1) - \alpha ) + (1-\lambda) (f_i (c_2) - \alpha) \\
	&= \lambda g_i(c_1) + (1-\lambda) g_i (c_2)
	\end{split}
	\end{equation*}
	
	Obtenemos que $ g_i $ es convexa para todo $ i = 1, ..., N $. Si usamos el Teorema de la Alternativa de Gordan obtenemos que solo se pueden dar una y solo de las siguientes posibilidades:
	
	\begin{itemize}
		\item[i)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{C}  \sum_{i=1}^{N}{t_i g_i}$.
		\item[ii)] $ \exists c \in C $ que cumple $ \max \lbrace g_1 (c), ..., g_N(c) \rbrace < 0 $.
	\end{itemize}
	
	Razonemos que no se puede dar $ ii) $. Si fuese así, tendríamos que $ \exists c \in C $ tal que $ \max \lbrace g_1(c), ..., g_N(c) \rbrace  =  \max \lbrace f_1(c) - \alpha, ..., f_N(c) - \alpha \rbrace < 0 $. En particular, existiría un índice $ j \in {1,...,N} $ que cumpliría $ f_j(c) - \alpha < 0 \Longrightarrow f_j(c) < \alpha = inf_{C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] $. Esto es imposible por la propia definición de ínfimo. Por ello, afirmamos que $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{C}  \sum_{i=1}^{N}{t_i g_i}$. Desarrollando el sumatorio:
	\begin{equation*}
	\begin{split}
	0 &< \inf_{C}  \sum_{i=1}^{N}{t_i g_i} = \inf_{C}  \sum_{i=1}^{N}{t_i(f_i - \alpha)} = \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)} - \sum_{i=1}^{N}{t_i\alpha} \right]\\
	&= \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)} -\alpha \sum_{i=1}^{N}{t_i} \right] = \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)} - \alpha \right] = \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)}\right] - \alpha
	\end{split}
	\end{equation*}
	
	Por lo tanto:
	\[
	0 < \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)}\right] - \alpha \Longleftrightarrow \inf_{C}\left[ \max \lbrace f_1, ..., f_N \rbrace \right] = \alpha  < \inf_{C} \left[ \sum_{i=1}^{N}{t_i(f_i)}\right]
	\]
	
	El lema \ref{lema2.1} nos aporta la otra desigualdad y llegamos nuevamente a que $ \exists \ttt \in \Delta_N $ que cumple:
		\[
	\inf_C\left[ \sum_{i=1}^{N} t_i f_i \right] = \inf_C \left[ \max \lbrace f_1, ..., f_N \rbrace \right] \]
	