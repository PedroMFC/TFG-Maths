\chapter{Lema de Simons y Teorema de la Alternativa de Gordan}
	\thispagestyle{empty}
	\paragraph{}Este capítulo se centra en lema de Simons, el cual será de gran ayuda para demostrar el Teorema de la Alternativa de Gordan.

	\paragraph{}Antes de comenzar, necesitamos hacer la siguiente definición.
	
	\begin{definicion}
		Dado $ N \in \NN $ con $ N > 1 $ llamamos símplex unitario de $ \RR^N $ al conjunto conexo y compacto de $ \RR^N $ dado por:
		\begin{equation*}
			\Delta_N := \left\lbrace \mathbf{t} \in \RR^N: \sum_{i=1}^{N}{t_i} = 1\quad , t_1,...,t_N \geq 0 \right\rbrace
		\end{equation*}
	\end{definicion}

	Por ejemplo, si $ N = 2 $, tenemos que $ \Delta_2 = co\{(1,0),(0,1)\}$, es decir es la envolvente convexa de los vectores de la base usual de $ \RR^2 $. De hecho en general, $ \Delta_N = co\{e_1,...e_N\} $.

	\paragraph{} Enunciamos este lema provio que usaremos posteriormente el la demostración del de Simons.
	
	\begin{lemaBox}\label{lema2.1}
		Sea $ N \in \NN $, con $ N \geq 1 $ y $ L: \RR^N \longrightarrow \RR $ definida por $ S(x):=\max\{x_1,...,x_N\}$. Entonces, $ S $ es sublineal. Además, si $ L:\RR^N \longrightarrow \RR $ es un funcional lineal tal que $ L \leq S $ entonces $ L $ es de la forma $ L(x) = t_1 x_1 + ... + t_N x_N $ con $ (t_1,...,t_N) \in \Delta_N$. De hecho, el recíproco también es cierto, es decir, si $ L =  (t_1,...,t_N) \in \Delta_N $ entonces $ L \leq S $.
		
		%%%%%% NOTA %%%%%%%%
		%% Las aplicaciones lineales coninciden con el dual del espacio. Es decir, podemos identificar la aplicación con sus coeficientes obeniendo de ese modo un punto del espacio.
		
	\end{lemaBox}
	\begin{proof}
		Claramente, $ S $ es positivamente homogénea. También es subaditiva ya que dados $ x,y \in \vecSpace $ : 
		\begin{equation*}
		\begin{split}
		S(x+y) &= \max \{x_1 + y_1, ..., x_N +y_N \}\\ 
		&\leq \max\{x_1, ..., x_N\} + \max \{y_1, ...,y_N\} = S(x) + S(y)
		\end{split}
		\end{equation*}
		
		Por ello, $ S $ es sublineal. Para terminar veamos que 
		\begin{equation*}
			\left\lbrace L:\RR^N \longrightarrow \RR : L \text{ lineal y }L\leq S \right\rbrace = \Delta_N
		\end{equation*}
		
		\begin{itemize}
			\item[$ \supseteq $ )] Sea $ \mathbf{t} \in \Delta_N$ definimos $ L:\RR^N \longrightarrow \RR $ como $ L(x):= \langle \mathbf{t},x \rangle $. Es evidente que $ L $ es lineal al serlo el producto escalar. Dado $ x \in \RR $:
			\begin{equation*}
				L(x) = \sum_{i=1}^{N}{t_i x_i} \leq \sum_{i=1}^{N}{t_i S(x)} = S(x)\sum_{i=1}^{N}{t_i} = S(x)
			\end{equation*}
			donde la primera desigualdad se debe a que $ x_i \leq S(x)$ para todo $ x_i$ con $ i=1,...,N $ y a que $ t_i \geq 0 $ ya que $ \mathbf{t} \in \Delta_N$. Esto también justifica la que última igualdad ya que $ \sum_{i=1}^{N}{t_i} = 1 $.
			
			\item[$ \subseteq $ )] Sea $ L =  (t_1,...,t_N) \in \RR^N $ lineal tal que $ \forall x \in \RR^N $ cumple $ \sum_{i=1}^{N}{t_i x_i} \leq \max\{x_1,...,x_N\}$. Así, si tomamos $ e_i \in \RR^N $ donde $ e_i $ representa el i-ésimo elemento de la base usual de $ \RR^N $ con $ i=1,...,N $. Entonces:   		
			\begin{equation*}
				L(-e_i) = -t_i \leq 0 \Longrightarrow t_i \geq 0 \quad \forall i=1,...,N
			\end{equation*}
			Si ahora llamamos $ e = \sum_{i=1}^{N}{e_i} $. Obtenemos:
			\begin{equation*}
				\begin{rcases*}
				L(e) = \sum_{i=1}^{N}{t_i} \leq \max \{1, ...,1\} = 1 \\
				L(-e) = -\sum_{i=1}^{N}{t_i} \leq \max \{-1, ...,-1\} = -1
				\end{rcases*} \Longrightarrow \sum_{i=1}^{N}{t_i} = 1
			\end{equation*}
			Concluimos entonces que $ L =  (t_1,...,t_N) \in \Delta_N $.
		\end{itemize}
		
	\end{proof}

	\paragraph{} Enunciamos ahora lema de Simons\cite{Simons2008} en que destacamos la ausencia de hipótesis topológicas, lo que será importante posteriormente.
	
	\begin{lemaBox}[Lema de Simons]\label{Simons}
		Sea $ C $ un subconjunto no vacío y convexo de un espacio vectorial. Dadas $ f_1, ..., f_N $ ($ N \geq 1 $)  funciones sobre $ C $ reales y convexas, entones existen $ (t_1, ..., t_N) \in \Delta_N $ que cumplen
		\[
		\inf_C\left[ \max \{f_1, ..., f_N\}\right] = \inf_C \left[ t_1 f_1+ ...+t_N f_N \right]
		\] 
	\end{lemaBox}
	\begin{proof}
		Sea  \vecSpace = $ \RR^N $ con $ N \in \NN $. Definimos $S:\vecSpace \longrightarrow P $ como \[ S(x_1, ..., x_N) := \max \{x_1, ..., x_N\} \]. Por el lema \ref{lema2.1} $ S $ es sublineal. Tomamos el subconjunto:
		\[ 
		D = \{ (x_1, ..., x_N)\in V: \exists c \in C \quad tal \quad que \quad \forall i = 1,...N,\quad f_i(c) \leq x_i \}
		\]
		
		$ D $ es un subconjunto convexo de \vecSpace. Sean $ x, y \in D $, por ello, existen $ c_x, c_y \in C $ tales que $ f_i (c_x) \leq x_i  $ y $ f_i (c_y) \leq y_i \quad \forall i=1,...,N $. Dado $ \lambda \in [0,1] $, llamamos $ c := (1-\lambda)c_x + \lambda c_y $ que pertenece a $ C $ por ser este convexo. Veamos que $ c $ es el elemento necesario de $ C $ para que cualquier combinación convexa de $ x $ e $ y $ esté en $ D $. Así, para todo $ i =1,...,N  $:	
		\[
		f_i(c) = f_i((1-\lambda)c_x + \lambda c_y) \leq (1-\lambda)f(c_x) + \lambda f(c_y) \leq (1-\lambda)x_i + \lambda y_i 
		\]
		donde la primera desigualdad se debe a que las $ f_i $ son convexas y la segunda a que $ x,y \in D $. Por ello, $ (1-\lambda)x_i + \lambda y_i \in D , \quad \forall \lambda \in [0,1] $ por lo que $ D $ es convexo. Aplicando el Teorema de Mazur-Orlizc, existe $ L $ funcinal lineal sobre \vecSpace tal que $ L \leq S $ e $ \inf_D L = \inf_D S $. \\
		
		Nuevamente, por el lema \ref{lema2.1} tenemos que $ L = (t_1,...,t_N) \in \Delta_N$. \\
		
		Finalmente:
		\[
		\inf_D L = \inf_{c\in C} \left[ t_1 f(c) + ..+ t_N f(c) \right] = \inf_{C} \left[ t_1 f + ..+ t_N f \right]
		\]
		y
		\[
		\inf_D S = \inf_{c\in C} \left[ \max \{f_1(c), ..., f_N(c)\} \right] = \inf_C\left[ \max \{f_1, ..., f_N\}\right] 
		\]
		por lo que 
		\[ \inf_{C} \left[ t_1 f + ..+ t_N f \right] = \inf_C\left[ \max \{f_1, ..., f_N\}\right]  \] 
	\end{proof}

	\paragraph{}Enunciamos ahora el Teorema de la alternativa de Gordan en su versión convexa.
	
	\begin{teoremaBox}[Teorema de la Alternativa de Gordan-versión convexa]\label{Gordan}
		Sea $ C $ un subconjunto convexo de un espacio vectorial y sean $ f_1,...,f_N : C \longrightarrow \RR $ funciones convexas ($ N \geq 1 $). Entonces una, y solo una, de la siguientes afirmaciones se cumple:
		\begin{itemize}
			\item[i)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{c \in C}  \sum_{i=1}^{N}{t_i f_i(c)}$.
			\item[ii)] $ \exists c \in C $ que cumple $ \max_{i=i,...,N}f_i (c) < 0 $.
		\end{itemize}
	\end{teoremaBox}
	\begin{proof}
		Si aplicamos el lema de Simons, lema \ref{Simons} a las funciones $ f_1,...,f_N $ obtenemos:
		
		\begin{equation*}
			\exists t \in \Delta_N \text{ : } \inf_{c \in C}\left[ \max_{i=1,...,N}f_i(c)\right] = \inf_{c \in C} \sum_{i=1}^{N}t_i f_i(c)
		\end{equation*}
		
		Supongamos en primer lugar que $ \alpha := \inf_{c \in C}\left[ \max_{i=1,...,N}f_i(c)\right] \in \RR $. Planteamos la siguiente alternativa, cuyos casos son excluyentes:
		\begin{itemize}
			\item[a)] $ 0 \leq \alpha $: implica $ i) $ ya que $ \alpha = \inf_{c \in C} \sum_{i=1}^{N}t_i f_i(c) $ 
			\item[b)] $ \alpha < 0 $: este caso, por su parte, implica $ ii) $ ya que:
			
			\begin{equation*}
				\alpha = \inf_{c \in C}\left[ \max_{i=1,...,N}f_i(c)\right] < 0 \Longleftrightarrow \exists c \in C  : \max_{i=i,...,N}f_i (c) < 0 
			\end{equation*}  
		\end{itemize}
	
		Para finalizar, veamos cuando $ \alpha =-\infty $. En este caso, estamos en la misma situación que en b) por lo que solo se puede dar $ ii) $.
	\end{proof}

	\paragraph{} Destacamos las siguientes observaciones:
	
	\begin{observacion}
		Esta versión convexa del teorema implica la versión clásica del mismo.
	\end{observacion}

	Dados $ \{x_1,...x_N\}$ con $ x_i \in \RR^M , (M \geq 1 )$ $i=1,...,N$, la versión clásica del teorema nos aporta las siguientes alternativas:

	\begin{itemize}
		\item[i*)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 = \sum_{i=1}^{N}{t_i x_i}$.
		\item[ii*)] $ \exists y \in \RR^M $ tal que cumple $ \max_{i=i,...,N} \langle y, x_i \rangle < 0 $.
	\end{itemize}

	Para ello, basta aplicar la versión convexa del teorema a $ C := \RR^M $ y a las funciones $ f_1,...,f_N : C \longrightarrow \RR $ definidas por $ f_i(c):=\langle c,x_i \rangle , \forall i=1,...,N  $. En este caso, la alternativa $ ii) $ implica $ ii*) $ ya que:
	
	\begin{equation*}
		\exists c \in C = \RR^M \text{ : } \max_{i=i,...,N} {\langle c,x_i \rangle}  =  \max_{i=i,...,N}f_i (c) < 0 
	\end{equation*}
	
	Por su parte, la alternativa $ i) $ nos da:
	\begin{equation*}
		\exists \mathbf{t} \in \Delta_N \text{ : } 0 \leq \inf_{c \in \RR^M}  \sum_{i=1}^{N}{t_i f_i(c) } = \inf_{c \in \RR^M} \sum_{i=1}^{N}{t_i\langle c,x_i \rangle} = \inf_{c \in \RR^M} \langle c, \sum_{i=1}^{N}{t_i x_i} \rangle
	\end{equation*}
	
	Si para $ c \in \RR^M $ se cumple que $ \langle c, \sum_{i=1}^{N}{t_i x_i} \rangle = 0 $ entonces podemos deducir que $ \sum_{i=1}^{N}{t_i x_i} = 0 $	ya que $ \sum_{i=1}^{N}{t_i x_i} \in (\RR^M)^{\perp} = \{0\} $. Así pues, tenemos que
	\begin{equation*}
		\text{Se cumple }i) \Longleftrightarrow \exists t \in \Delta_N \text{ : }  0 = \sum_{i=1}^{N}{t_i x_i}
	\end{equation*}
	
	Es claro entonces que obtenemos $ i*) $.
	
	\begin{observacion}
		El lema de Simons(lema \ref{Simons}) y el Teorema de la Alternativa de Gordan (teorema \ref{Gordan}) son equivalentes.
	\end{observacion}

	\paragraph{} Ya hemos visto que el Lema de Simons implica el Teorema de la Alternativa de Gordan. Veamos que el recíproco también es cierto.  
	COMPLETAR.