\chapter{Lema de Simons y Teorema de la Alternativa de Gordan}
	\thispagestyle{empty}
	\newcommand{\ttt}{\textbf{\emph{t}}}
	\newcommand{\sss}{\textbf{\emph{s}}}
	\newcommand{\xx}{\textbf{\emph{x}}}
	\newcommand{\yy}{\textbf{\emph{y}}}
	
	\paragraph{}Este capítulo se centra en lema de Simons, el cual será de gran ayuda para demostrar el Teorema de la Alternativa de Gordan.

	\paragraph{}Antes de comenzar, necesitamos hacer la siguiente definición. Para hacer más ágil la lectura, dado $ N\in \NN $ en las N-uplas y espacios vectoriales $ \RR^N $ supondremos que $ N \geq 1 $. También notaremos con caracteres en negrita a los elementos de espacios vectoriales reales, por ejemplo, $  \ttt = (t_1,...,t_N)\in \RR^N$. 
	
	\begin{definicion}
		Dado $ N \in \NN $ llamamos símplex unitario de $ \RR^N $ al conjunto convexo y compacto de $ \RR^N $ dado por:
		\begin{equation*}
			\Delta_N := \left\lbrace \ttt \in \RR^N: \sum_{i=1}^{N}{t_i} = 1, \hspace{0.5em} t_1,...,t_N \geq 0 \right\rbrace .
		\end{equation*}
	\end{definicion}

	Recordamos ahora la envolvente de un conjunto cualquiera $ X $, que notamos como $ \mathrm{co}(X) $ y que se define como la intersección de todos los conjuntos convexos que contienen a $ X $. En el caso de los símplex, si $ N = 2 $, tenemos que $ \Delta_2 = \mathrm{co} \{(1,0),(0,1)\}$ (envolvente convexa de los vectores de la base usual de $ \RR^2 $). De hecho en general, $ \Delta_N = co\{e_1,...e_N\} $. \\
	
	Antes de continuar veamos que efectivamente $ \Delta_N $ es convexo y compacto:
	
	\begin{itemize}
		\item  Convexo: tenemos que comprobar que dados $ \ttt, \sss \in \Delta_N  \Longrightarrow \lambda\ttt + (1-\lambda)\sss \in \Delta_N, \lambda \in \lbrack 0,1 \rbrack$. En efecto, las coordenadas de $ \lambda\ttt + (1-\lambda)\sss \in \RR^N $ verifican:
		\begin{itemize}
			\item [i) ] $ \lambda t_i + (1-\lambda)s_i \geq 0 $ para todo $ i = 1,..., N $ ya que $ t_i, s_i \geq 0 $.
			\item [ii) ] $ \sum_{i=1}^{N} (\lambda t_i + (1-\lambda)s_i) =   \lambda \sum_{i=1}^{N} t_i + (1-\lambda)\sum_{i=1}^{N} s_i = \lambda + (1 - \lambda) = 1$ ya que $ \sum_{i=1}^{N} t_i = \sum_{i=1}^{N} s_i = 1  $.
		\end{itemize}
	
		Por lo tanto, $ \lambda\ttt + (1-\lambda)\sss \in \Delta_N \text{ para todo }\lambda \in \lbrack 0,1 \rbrack $.
		
		\item Compacto: al encontrarnos en $ \RR^N $ y aplicando el conocido Teorema de Heine-Borel basta ver que $ \Delta_N $ es cerrado y acotado. Claramente es acotado por lo que nos centraremos en la de cerrado. Sea $ \lbrace \ttt_n \rbrace_{ n \in \NN} $ una sucesión de $ \Delta_N $ y sea $ \ttt_0 \in \RR^N $ tal que $ \lbrace \ttt_n \rbrace_{ n \in \NN} \longrightarrow \ttt_0 $. Tenemos que comprobar que $ \ttt_0 \in \Delta_N $. 
		\begin{itemize}
		\item[i) ] Como todas las coordenadas de cada $ \ttt_n $ para $ n \in \NN $ son no negativas podemos asegurar que las de $ \ttt_0 $ también lo son.
		\item[ii) ] La función $ f: \RR^N \longrightarrow \RR $ definida como $ f(\ttt) =  \sum_{i=1}^{N} t_i $ es continua. Claramente $ f(\ttt) = 1 $ para todo $ \ttt \in \Delta_N $ y por ello $ \lbrace f(\ttt_n) \rbrace_{ n \in \NN} \longrightarrow 1$. Por continuidad de $ f $ y unicidad de límite tenemos que $ f(\ttt_0) = 1 $ pero eso implica que la suma de sus componentes vale 1.
		\end{itemize} 
	
		Así, hemos demostrado que $ \ttt_0 \in \Delta_N $ y por lo tanto $ \Delta_N $ es compacto. 
	\end{itemize}

	
	\paragraph{}Antes de continuar, notamos que el espacio vectorial de todas las aplicaciones lineales de $ \RR^N $ a $ \RR $, con $ N \in \NN $, se puede identificar con $ \RR^N $. Esto se debe a que si $ L:\RR^N \longrightarrow \RR $ es lineal, entonces es de la forma $ L(\xx) = a_1x_1 + \cdots + a_Nx_N $ que se corresponde con $ \textbf{\emph{a}} = (a_1,\dots,a_N) \in \RR^N $. Del mismo modo, dado el vector tenemos la aplicación lineal asociada. En definitiva, el dual de 
	$ \RR^N $, que notamos como, $ (\RR^N)^* $, lo podemos identificar con $ \RR^N $ .
	\paragraph{} También presentamos el producto escalar de dos vectores de $ \RR^N $ que definimos como:
	\[
	\begin{split}
	\langle \cdot, \cdot \rangle : \RR^N \times \RR^N \longrightarrow &\RR \\
	(\xx, \yy) \longmapsto &\langle \xx, \yy. \rangle = \sum_{i=0}^{N}x_iy_i
	\end{split}
	\]
	\paragraph{}Enunciamos este lema previo que usaremos posteriormente en la demostración del de Simons.
	\begin{lemaBox}\label{lema2.1}
		Sea $ N \in \NN $ y $ S: \RR^N \longrightarrow \RR $ definida por: \[ S(\xx):=\max\{x_1,...,x_N\}.\] Entonces, $ S $ es sublineal. Además, si $ L:\RR^N \longrightarrow \RR $ es un funcional lineal tal que $ L \leq S $ entonces $ L $ es de la forma $ L(\xx) = t_1 x_1 + ... + t_N x_N $ con $ (t_1,...,t_N) \in \Delta_N$. De hecho, el recíproco también es cierto, es decir, si $ L = \ttt = (t_1,...,t_N) \in \Delta_N $ entonces $ L \leq S $.
		
		%%%%%% NOTA %%%%%%%%
		%% Las aplicaciones lineales coninciden con el dual del espacio. Es decir, podemos identificar la aplicación con sus coeficientes obeniendo de ese modo un punto del espacio.
		
	\end{lemaBox}
	\begin{proof}
		Claramente, $ S $ es positivamente homogénea. También es subaditiva ya que dados $ x,y \in \vecSpace $ : 
		\begin{equation*}
		\begin{split}
		S(\xx+\yy) &= \max \{x_1 + y_1, ..., x_N +y_N \}\\ 
		&\leq \max\{x_1, ..., x_N\} + \max \{y_1, ...,y_N\} = S(\xx) + S(\yy).
		\end{split}
		\end{equation*}
		
		Por ello, $ S $ es sublineal. Para terminar veamos que  \[\left\lbrace L:\RR^N \longrightarrow \RR : L \text{ lineal y }L\leq S \right\rbrace  =  \Delta_N \]
		 a través de la correspondencia mostrada anteriormente entre $ \RR^N $ y su dual.
		
		\begin{itemize}
			\item[$ \supseteq $ )] Sea $ \mathbf{t} \in \Delta_N$ definimos $ L:\RR^N \longrightarrow \RR $ como $ L(\xx):= \langle \mathbf{t},\xx \rangle $. Es evidente que $ L $ es bilineal al serlo el producto escalar. Dado $ \xx \in \RR^N $:
			\begin{equation*}
				L(\xx) = \sum_{i=1}^{N}{t_i x_i} \leq \sum_{i=1}^{N}{t_i S(\xx)} = S(\xx)\sum_{i=1}^{N}{t_i} = S(\xx)
			\end{equation*}
			donde la primera desigualdad se debe a que $ x_i \leq S(\xx)$ para todo $ x_i$ con $ i=1,...,N $ y a que $ t_i \geq 0 $ ya que $ \mathbf{t} \in \Delta_N$. Esto también justifica la que última igualdad ya que $ \sum_{i=1}^{N}{t_i} = 1 $.
			
			\item[$ \subseteq $ )] Sea $ L = \ttt = (t_1,...,t_N) \in \RR^N $ lineal tal que para todo $ \xx \in \RR^N $ cumple que $ \sum_{i=1}^{N}{t_i x_i} \leq \max\{x_1,...,x_N\}$. Así, si tomamos $ e_i \in \RR^N $ donde $ e_i $ representa el i-ésimo elemento de la base usual de $ \RR^N $ con $ i=1,...,N $, entonces:   		
			\begin{equation*}
				L(-e_i) = -t_i \leq 0 \Longrightarrow t_i \geq 0 \quad \forall i=1,...,N
			\end{equation*}
			Si ahora llamamos $ e = \sum_{i=1}^{N}{e_i} $. Obtenemos:
			\begin{equation*}
				\begin{rcases*}
				L(e) = \sum_{i=1}^{N}{t_i} \leq \max \{1, ...,1\} = 1 \\
				L(-e) = -\sum_{i=1}^{N}{t_i} \leq \max \{-1, ...,-1\} = -1
				\end{rcases*} \Longrightarrow \sum_{i=1}^{N}{t_i} = 1
			\end{equation*}
			Concluimos entonces que $ L = \ttt = (t_1,...,t_N) \in \Delta_N $.
		\end{itemize}
		
	\end{proof}

	\paragraph{} Enunciamos ahora lema de Simons\cite{Simons2008} en que destacamos la ausencia de hipótesis topológicas, lo que será importante posteriormente.
	
	\begin{lemaBox}[Lema de Simons]\label{Simons}
		Sea $ C $ un subconjunto convexo de un espacio vectorial. Dadas $ f_1, \dots, f_N : C \longrightarrow \RR $, con $ N \in \NN $, funciones convexas, entones existe $ \ttt \in \Delta_N $ que cumplen
		\[
		\inf_{c \in C}\left[ \max_{i=1\dots,N } \{f_i(c)\}\right] = \inf_{c \in C} \left[ \sum_{i=1}^{N} t_i f_i(c) \right]
		\] 
	\end{lemaBox}
	\begin{proof}
		Sea  \vecSpace = $ \RR^N $ con $ N \in \NN $. Definimos $S:\vecSpace \longrightarrow \RR $ como \[ S(x_1, ..., x_N) := \max \{x_1, ..., x_N\}. \] Por el lema \ref{lema2.1}, $ S $ es sublineal. Tomamos el subconjunto:
		\[ 
		D = \{ (x_1, ..., x_N)\in V: \exists c \in C \quad \text{ tal que } \quad \forall i = 1,...N,\quad f_i(c) \leq x_i \}
		\]
		
		$ D $ es un subconjunto convexo de \vecSpace. Sean $ x, y \in D $, por ello, existen $ c_x, c_y \in C $ tales que $ f_i (c_x) \leq x_i  $ y $ f_i (c_y) \leq y_i \quad \forall i=1,...,N $. Dado $ \lambda \in [0,1] $, llamamos $ c := (1-\lambda)c_x + \lambda c_y $ que pertenece a $ C $ por ser este convexo. Veamos que $ c $ es el elemento necesario de $ C $ para que cualquier combinación convexa de $ x $ e $ y $ esté en $ D $. Así, para todo $ i =1,...,N  $:	
		\[
		f_i(c) = f_i((1-\lambda)c_x + \lambda c_y) \leq (1-\lambda)f(c_x) + \lambda f(c_y) \leq (1-\lambda)x_i + \lambda y_i 
		\]
		donde la primera desigualdad se debe a que las $ f_i $ son convexas y la segunda a que $ x,y \in D $. Por ello, $ (1-\lambda)x_i + \lambda y_i \in D , \quad \forall \lambda \in [0,1] $ por lo que $ D $ es convexo. Aplicando el Teorema de Mazur-Orlizc, existe $ L $ funcinal lineal sobre \vecSpace tal que $ L \leq S $ e $ \inf_D L = \inf_D S $. \\
		
		Nuevamente, por el lema \ref{lema2.1} tenemos que $ L = \ttt \in \Delta_N$. \\
		
		Finalmente:
		\[
		\inf_D L = \inf_{c\in C} \left[ t_1 f(c) + \cdots+ t_N f(c) \right] =\inf_{c \in C} \left[ \sum_{i=1}^{N} t_i f_i(c) \right]
		\]
		y
		\[
		\inf_D S = \inf_{c\in C} \left[ \max \{f_1(c), \dots, f_N(c)\} \right] = \inf_{c \in C}\left[ \max_{i=1\dots,N } \{f_i(c)\}\right]
		\]
		por lo que 
		\[ \inf_{c \in C}\left[ \max_{i=1\dots,N } \{f_i(c)\}\right] = \inf_{c \in C} \left[ \sum_{i=1}^{N} t_i f_i(c) \right] \] 
	\end{proof}

	Enuciamos ahora el Teorema de la Alternativa de Gordan en su versión clásica.
	\begin{teoremaBox}[Teorema de la alternativa de Gordan]\label{GordanClasic}
		Sean $ \{\xx_1,...\xx_N\}$ con $ \xx_i \in \RR^M \text{ para } i=1,...,N$ y $ N, M \in \NN $. Entonces una, y solo una, de la siguientes afirmaciones se cumple:
		
		\begin{itemize}
			\item[i*)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 = \sum_{i=1}^{N}{t_i \xx_i}$.
			\item[ii*)] $ \exists \yy \in \RR^M $ tal que cumple $ \max_{i=1,...,N} \langle \yy, \xx_i \rangle < 0 $.
		\end{itemize}
	\end{teoremaBox}

	\paragraph{} Omitimos esta demostración ya que a continuación mostramos la versión convexa del mismo y será la que probaremos. Después veremos que la versión convexa  implica la clásica por lo que quedará probada.
	
	\begin{teoremaBox}[Teorema de la Alternativa de Gordan-versión convexa]\label{Gordan}
		Sea $ C $ un subconjunto convexo de un espacio vectorial y sean $ f_1,...,f_N : C \longrightarrow \RR $ funciones convexas ($ N \geq 1 $). Entonces una, y solo una, de la siguientes afirmaciones se cumple:
		\begin{itemize}
			\item[i)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{c\in C}  \sum_{i=1}^{N}{t_i f_i (c)}$.
			\item[ii)] $ \exists c \in C $ que cumple $\max_{i=1\dots,N } \{f_i(c)\} < 0 $.
		\end{itemize}
	\end{teoremaBox}
	\begin{proof}
		Si aplicamos el lema de Simons, lema \ref{Simons}, a las funciones $ f_1,...,f_N $ obtenemos:
		
		\begin{equation*}
			\exists t \in \Delta_N \text{ : } \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] = \inf_{c \in C} \sum_{i=1}^{N}t_i f_i (c).
		\end{equation*}
		Supongamos en primer lugar que $ \alpha := \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] \in \RR $. Planteamos la siguiente alternativa, cuyos casos son excluyentes:
		\begin{itemize}
			\item[a)] $ 0 \leq \alpha $: implica $ i) $ ya que $ \alpha = \inf_{c \in C} \sum_{i=1}^{N}t_i f_i (c) $.
			\item[b)] $ \alpha < 0 $: este caso, por su parte, implica $ ii) $ ya que:
			
			\begin{equation*}
				\alpha =\inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] < 0 \Longleftrightarrow \exists c \in C  :\max_{i=1\dots,N } \{f_i(c)\} < 0. 
			\end{equation*}  
		\end{itemize}
		Para finalizar, veamos cuando $ \alpha =-\infty $. En este caso, estamos en la misma situación que en b) por lo que solo se puede dar $ ii) $.
	\end{proof}

	\paragraph{} Destacamos las siguientes observaciones:
	
	\begin{observacion}
		Esta versión convexa del teorema implica la versión clásica del mismo.
	\end{observacion}

	Para ello, basta aplicar la versión convexa del teorema a $ C := \RR^M $ y a las funciones $ f_1,...,f_N : C \longrightarrow \RR $ definidas por $ f_i(c):=\langle c, \xx_i \rangle , \forall i=1,...,N  $. Notar que las funciones $ f_1,...,f_N $ son lineales por la izquierda y como consecuencia son convexas. En este caso, la alternativa $ ii) $ implica $ ii*) $ ya que:
	
	\begin{equation*}
		\exists c \in C = \RR^M \text{ : } \max_{i=1,...,N} {\langle c,x_i \rangle}  =  \max_{i=1,...,N}f_i (c) < 0 .
	\end{equation*}
	Por su parte, la alternativa $ i) $ nos da:
	\begin{equation*}
		\exists \mathbf{t} \in \Delta_N \text{ : } 0 \leq \inf_{c \in \RR^M}  \sum_{i=1}^{N}{t_i f_i(c) } = \inf_{c \in \RR^M} \sum_{i=1}^{N}{t_i\langle c, \xx_i \rangle} = \inf_{c \in \RR^M} \langle c, \sum_{i=1}^{N}{t_i 	\xx_i} \rangle. 
	\end{equation*}
	Hemos obtenido por ello que  $0  \leq \inf_{c \in \RR^M} \langle c, \sum_{i=1}^{N}{t_i \xx_i} \rangle  $ lo que nos lleva a $ 0 \leq \langle c, \sum_{i=1}^{N}{t_i \xx_i} \rangle  $ para todo $ c \in \RR^M $. Usando la bilinealidad del producto escalar:
	\[
	0 \leq \langle -c, \sum_{i=1}^{N}{t_i \xx_i} \rangle \Longleftrightarrow 	0 \leq -\langle c, \sum_{i=1}^{N}{t_i \xx_i} \rangle 
	\Longleftrightarrow  \langle c, \sum_{i=1}^{N}{t_i \xx_i}\rangle \leq 0, \quad \forall c \in \RR^M.
	\]
	Juntando ambas desigualdades obtenemos que $ 0 =  \langle c, \sum_{i=1}^{N}{t_i \xx_i}\rangle, \quad \forall c \in \RR^M $. Como la igualdad anterior se cumple para todo elemento de $ \RR^M $ entonces podemos deducir que $ \sum_{i=1}^{N}{t_i \xx_i} = 0 $	ya que $ \sum_{i=1}^{N}{t_i \xx_i} \in (\RR^M)^{\perp} = \{0\} $. Así pues, tenemos que
	\begin{equation*}
		\text{se cumple }i) \Longleftrightarrow \exists t \in \Delta_N \text{ : }  0 = \sum_{i=1}^{N}{t_i \xx_i}.
	\end{equation*} 	
	
	Es claro entonces que obtenemos $ i*) $.
	
	\begin{observacion}
		El lema de Simons(lema \ref{Simons}) y el Teorema de la Alternativa de Gordan (teorema \ref{Gordan}) son equivalentes.
	\end{observacion}

	\paragraph{} Ya hemos visto que el Lema de Simons implica el Teorema de la Alternativa de Gordan. Veamos que el recíproco también es cierto.  \\
	
	Llamamos $ \alpha := \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] $. Si $ \alpha = -\infty $. Por el lema \ref{lema2.1} sabemos que $ \forall \ttt \in \Delta_N $ se cumple que $ \sum_{i=1}^{N} t_i f_i(c) \leq \max_{i=1\dots,N } \{f_i(c)\}$ para todo $ c \in C$. Tomando ínfimos en C:
	\[
	\inf_{c \in C}\left[ \sum_{i=1}^{N} t_i f_i(c) \right] \leq \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] = -\infty \Longrightarrow \inf_{ c \in C}\left[ \sum_{i=1}^{N} t_i f_i (c)\right] = -\infty 
	\]
	
	y por ello $ \forall \ttt \in \Delta_N $ (en particular para uno) se cumple que
	
	\[
	\inf_{c \in C}\left[ \sum_{i=1}^{N} t_i f_i (c) \right] = \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right]. \]
	Supongamos ahora que $ \alpha \in \RR $. Sean las funciones $ q_1, ..., g_N: C \longrightarrow \RR $ definidas como $ g_i = f_i - \alpha $ con $ i=1,...,N$. Veamos que las funciones $ g_1, ..., g_N $ son convexas como consecuencia de que $ f_1, ..., f_N $ lo son. Sean $ c_1,c_2 \in C $ y $ \lambda \in \left[0,1\right] $:
	\begin{equation*}
	\begin{split}
	g_i(\lambda c_1 + (1-\lambda) c_2) &= f_i(\lambda c_1 + (1-\lambda) c_2) - \alpha \\
	&\leq \lambda f_i(c_1) + (1-\lambda)f_i(c_2) - \alpha \\
	&= \lambda f_i(c_1) + (1-\lambda)f_i(c_2) - \lambda \alpha + (1-\lambda)\alpha \\
	&= \lambda( f_i(c_1) - \alpha ) + (1-\lambda) (f_i (c_2) - \alpha) \\
	&= \lambda g_i(c_1) + (1-\lambda) g_i (c_2).
	\end{split}
	\end{equation*}
	
	Obtenemos que $ g_i $ es convexa para todo $ i = 1, ..., N $. Si usamos el Teorema de la Alternativa de Gordan obtenemos que solo se pueden dar una y solo de las siguientes posibilidades:
	
	\begin{itemize}
		\item[i)] $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{c \in C}  \sum_{i=1}^{N}{t_i g_i (c)}$.
		\item[ii)] $ \exists c \in C $ que cumple $ \max_{i=1\dots,N } \{g_i(c)\}  < 0 $.
	\end{itemize}
	
	Razonemos que no se puede dar $ ii) $. Si fuese así, tendríamos que $ \exists c \in C $ tal que $ \max_{i=1\dots,N } \{g_i(c)\} =  \max_{i=1\dots,N } \{f_i(c) - \alpha \} < 0 $. En particular, existiría un índice $ j \in {1,...,N} $ que cumpliría $ f_j(c) - \alpha < 0 \Longrightarrow f_j(c) < \alpha = \inf_{ c\in C}\left[\max_{i=1\dots,N } \{f_i(c)\} \right] $. Esto es imposible por la propia definición de ínfimo. Por ello, afirmamos que $ \exists \mathbf{t} \in \Delta_N $ tal que $ 0 \leq \inf_{C}  \sum_{i=1}^{N}{t_i g_i}$. Desarrollando el sumatorio:
	\begin{equation*}
	\begin{split}
	0 &< \inf_{c \in C}  \sum_{i=1}^{N}{t_i g_i (c)} = \inf_{c \in C}  \sum_{i=1}^{N}{t_i(f_i(c) - \alpha)} = \inf_{c |in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)} - \sum_{i=1}^{N}{t_i\alpha} \right]\\
	&= \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)} -\alpha \sum_{i=1}^{N}{t_i} \right] = \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)} - \alpha \right] = \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)}\right] - \alpha.
	\end{split}
	\end{equation*}
	Por lo tanto:
	\[
	0 < \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)}\right] - \alpha \Longleftrightarrow \inf_{c \in C}\left[ \max_{i=1\dots,N } \{f_i(c)\}\right] = \alpha  < \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)}\right].
	\]
	El lema \ref{lema2.1} nos aporta la otra desigualdad y llegamos nuevamente a que $ \exists \ttt \in \Delta_N $ que cumple:
	\[
	\inf_{c \in C}\left[ \max_{i=1\dots,N } \{f_i(c)\}\right] = \inf_{c \in C} \left[ \sum_{i=1}^{N}{t_i f_i(c)}\right]. \]
	