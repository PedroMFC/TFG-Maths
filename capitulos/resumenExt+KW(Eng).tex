\chapter{Resumen extendido y palabras clave en inglés}
In Mathematics, the end of degree project that we present is an incursion into a field included in optimization, the theorems of the alternative, and their applications, mainly to the own optimization and finances.\\

Many of the theorems of the alternative are just reformulations of convex separation's theorems in certain contexts, so we start this memory with a study on Hahn-Banach's theorem. There are several equivalent versions of this result, collected for example in \cite{schechter1996handbook}. That is the reason why many authors talk about Hahn-Banach's theorems. We will focus on one of them, Mazur-Orlicz-König's theorem. Despite its geometrical charge, it is mainly an algebraic result. We will provide a proof of that theorem -- the results in this work are self-contained --  and we will use it to give a convex version  of Gordan's theorem of alternative. Actually, it is an equivalent result due to S. Simons. This version is more general than the orginal Gordan's result and it will be extremely useful in the next important results. \\ 

Then, we will apply the convex Gordan's theorem of the alternative in the minimax theory. A minimax inequality guarantees, under certain hypothesis, that in a two variables function we can substitute inf\hspace{0.5mm}sup for sup\hspace{0.5mm}inf. The power of minimax inequalities is clear when we use it -- in one that we will deduce from the convex Gordan's theorem -- to give classical convex separation's results. \\

Next, we will deduce from one separation's theorem the Farkas's lemma, which is one of the most known theorem of the alternative. This will allow us to prove, almost immediately, one of the key points of linear programming: the duality theorem. Considering more general optimization theorems, those in which the objective function and the inequalities constraints are differentiables, we will establish, using Gordan's theorem of the alternative, the theorems of Fritz John and Karush-Kuhn-Tucker.\\

We will conclude with a little foray into the field of financial mathematics. After introducing some main concepts, like a derivate security; we will prove, as a consequence of one of the convex separation's theorem, the result known as ``First Fundamental Theorem of Asset Pricing''. It can be used in the pricing of European's options in the binomial model. A European's option is a contract that gives the owner the right, but not the obligation, to buy or sell certain asset. This situation is a clear advantage for the buyer because he can earn money without taking any risk, breaking the no-arbitrage principle. We solve this inconvenient imposing a price to get the option as we mentioned before. Finally, we have programmed using \textit{Sagemath 2.8} some examples in order to know how their price varies according to its parameters. For instance, we can study its price depending on considerations whether the asset's price goes up or down or even the value of the money in the future, choosing the most profitable option. During this process, we have made some considerations that model our market. We can remark the positivity of prices which guarantees that all the prices are positive, divisibility which refers to the fact that one can hold a fraction of an asset or liquidity, meaning that any asset can be bought or sell on demand at the market. Some assumptions, like liquidity, are more mathematical than practical -- in real markets there are bounds on the volume of trading. \\

Related to Computer Science, this project consists in a study of 3D digitalization process, which is considered part of Computer Graphics. First, in order to have a digital copy of some desired object, we must obtain some point clouds representing it. They are like three-dimesional photographs. Using this analogy, one photograph does not cover the whole object's surface. That is the reason why we need a few clouds for each one. Unfortunately, the point clouds are not in the same reference system because we must move either the object or the scanner to get all the information. We need to ``modify'' them, so they match and finally obtain a 3D copy of the object. This modification or treatment have three different processes: alignment, meshing and triangulation. The first one refers to the fact that we have mentioned before, the point clouds are not in the same reference system. This step solves this problem. To accomplish this target properly, our point clouds need some overlapping to know how to place them exactly. This situation leads us to the meshing step. Due to overlapping, once we have the shots expressed in the same coordinates, some parts have a higher level of points density. With meshing, we refer to omit ``repeated'' points in these zones that has been registered several times in order to get a good alignment. At this moment, we have the object represent by the points. The goal of triangulating is just getting the surface represented by the points. The three steps that we have just indicated, are very interesting and they deserve a complete study to solve them. However, in this project we will focus on the clouds' alignment, also called data registration. \\

The first algorithm we will study is the Iterative Closest Point (ICP). It is considered a generic algorithm because it can be used to align two points clouds independently of the characteristics of the object. We will talk about this topic lately in this work. However, it is important to highlight that there is not a suitable algorithm to align any shots from all type of objects. This section starts with a brief introduction to quaternions. Quaternios are similar to an extension of complex numbers. In our case, we will use them to represent rotations in three dimensions (a rotation and a translation are needed to align the clouds). There are other  ways to represent them such as $ 3 \times 3$ arrays or Euler angles but quaternios have some advantages. After that, we will discuss how to obtain the matrix and the translation commented before by optimizing a distance function. This will give us the steps we must follow in the algorithm. We will also prove the local convergence of this method. The fact that it converges locally forces us to have a pre-alignmet step. With pre-align we refer to place both point clouds close enough  to ensure that the result is correct. Another disadvantage of ICP is the quadratric complexity in time which make it untreatable with large point clouds. \\

Our next goal will be to reduce the amount of time spent by the algorithm to complete. We will achieve it by reducing the elements in the clouds and just using the most relevant points of the model in our method. To detect these points, called key points, we will use the variation of the normal vector to distinguish between planar and sharp areas. It is possible thanks to the scanner used to get the shoots. The output format gives the points in a matrix so we can obtain neighborhood relations. This decrement of the points' quantity will cause a significant reduction in the execution time. Finally, we will study how affects the pre-alignment in the provided results.  \\

The second algorithm we will discuss is the Random Sample Consensus (RANSAC). It is used to get the parameters that fit a mathematical model given by some observations. It is considered a robust algorithm even in the presence of outliers and is based on a hypothesis-test paradigm. In our case, we will use it to detect planes in our model. The steps to obtain the planes are very simple: select three random points that will represent a plane, count how many points fits that plane (with some tolerance), repeat this process $ N $ times and take the one that has the maximum number of points. We will give a formula to determine the number $ N $ depending on the probability to be an outlier, the desired probability to find the model and the number of parameters needed to define it. Once we have the planes, we will calculate their intersections in order to have significant corners. Then, we will use the angles between the planes to assign a descriptor to the intersection points and determine which of them match in different points clouds.\\

To test the proposed methods, we have different models: a clay feet property of the Faculty of Fine Arts of Granada, a part of \textit{La torre de las Gallinas} in the Alhambra and an office. These models have been taken with a Faro Focus 130 laser scanner. It is has also been necessary to develop an application in which we can code and observe the results of the different algorithms. We will cover this part on the last chapter. This program is written in \textit{C++}, we use \textit{OpenGL} as the graphic library and \textit{Qt} for the user interface. During its develepment, we have applied several concepts related to Computer Graphics. For instance, methods to select or delete a bunch of points or pick one of them. We have also been concerned about the application performance, especially in our case where we use models with a lot of vertices.\\

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Keywords: }} #1
}
\keywords{theorems of the alternative, Hahn-Banach's theorem, minimax, optimization, financial mathematics, data registration, quaternions, ICP, RANSAC.}