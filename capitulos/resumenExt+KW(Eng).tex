\chapter{Resumen extendido y palabras clave en inglés}
In Mathematics, the end of degree project that we present is an incursion into a field included in optimization, the theorems of the alternative, and their applications, mainly to the own optimization and finances.\\

Many of the theorems of the alternative are just reformulations of convex separation's theorems in certain contexts, so we start this memory with a study on Hahn-Banach's theorem. There are several equivalent versions of this result, collected for example in \cite{schechter1996handbook}. That is the reason why many authors talk about Hahn-Banach's theorems. We will focus on one of them, Mazur-Orlicz-König's theorem. Despite its geometrical charge, tt is mainly an algebraical result. We will provide a proof of that theorem -- the results in this work are self-contained --  and we will use it to give a convex version  of Gordan's theorem of alernative. Actually, it is an equivalent result due to S. Simons. This version is more general than the orginal Gordan's result and it will be extremmely useful in the next important results. \\

Then, we will apply the convex Gordan's theorem of the alternative in the minimax theory. A minimax inequality guarantees, under certain hypothesis, that in a two variables function we can substitute inf\hspace{0.5mm}sup for sup\hspace{0.5mm}inf. The power of minimax inequalities is clear when we use it -- in one that we will deduce from the convex Gordan's theorem -- to give classical convex separation's results. \\

Next, we will deduce from one separation's theorem the Farkas's lemma, which is one of the most known theorem of the alternative. This will allow us to prove, almost immediately, one of the key points of linear programming: the duality theorem. Considering more general optimization theorems, those in which the objective function and the inequalities constraints are differentiables, we wil establish, using Gordan's theorem of the alternative, the theorems of Fritz John and Karush-Kuhn-Tucker.\\

We will conclude with a little foray into the field of financial mathematics. After introducing some of the main concepts, like a derivate security; we will prove, as a consequence of one of the convex separation's theorem, the result kwonw as ``First Fundamental Theorem of Asset Pricing''. It can be used in the pricing of European's options in the binomial model. Finally, we have programmed using \textit{Sagemath 2.8} some examples in order to know how their price varies according to its parametres.\\

Related to Computer Science, this project consists in a study of 3D digitalization process, which is considered part of Computer Graphics. First, in order to have a digital copy of some desired object, we must obtain some point clouds representig it. They are like three-dimesional photographies. Using this analogy, one photograph does not coverd the whole object's surface. That is the reason why we need a few clouds for each one. Unfortunately, the point clouds are not in the same reference system. We need to ``modify'' them so they match and finally obtain a 3D copy of the object. This modification or treatment have three different processes: alignment, meshing and triangulation. The first one refers to the fact that we mentioned before, the points clouds are not in the same reference system. This step solves this problem. To accomplish this target in a proper way, our point clouds need some overlapping to now exactly how to place them. This situation leads us to the meshing step. Due to overlapping, once we have the shots expressed in the same coordinates, some parts have a higher level of points density. With meshing, we refer to ommit ``repeated'' points in theese zones that has been registered several times to get a good aligment. In this moment, we have the object represent by the points. The goal of triangulating is just getting the surface represented by the points. The three steps that we have just mentioned, are very interesting and they deserve a complete study to solve them. However, in this project we will focus in the clouds alignment, also called data registration. \\

The fisrt algorithm we will study is the Iterative Closest Point (ICP). It is considered a generic algorithm because it can be used to align two points clouds independenly of the characteristics of the object. We will talk about this topic lately in this work. However, it is important to highlight that there is no suitable algorithm to align any shots from all type of objects. This section starts with a brief introduction to quaternions. Quaternios are like an extension of complex numbers. In our case, we will use them to represent rotations in three dimensions (a rotation and a translation are needed to align the clouds). There are other  ways to represent them such as $ 3 \times 3$ matrixes or Euler angles but quaternios have some advantges. After that, we will disccus how to obtain the matrix and the translation mentioned before by optimizating a distance fucntion. This will give us the steps we must follow in the algorithm. We will also proof the local convergence of this method. The fact that it converges locally forces us to have a pre-alignmet step to ensure that the result is correct. Another disadvantage of ICP is the quadatric complexity in time which make it untreatable with large point clouds. \\

Our next goal will be to reduce the amount of time spent by the algorithm to complete. We will achieve it by reducing the point clouds and just using the most relevant of the model in our method. To detect theese points, called key points, we will use the variation of the normal vector to distinguish between planar and sharp areas. It is posible thanks to the scaner used to get the shoots. This decrement of the points' quantity will cause a significant reduction in the exuction time. Finally, we will study how affects the pre-alignment in the provided results.  \\

The second algorithm we will disccus is the Random Sample Consensus (RANSAC). It is used to get the parameters that fit a mathematical model given by some observations. It is considered a robust algorithm even in the presence of outliers and is based on an hypothesis-test paradigm. In our case, we will use it to detect planes in our model. The steps to obtain the planes are very simple: select three random points that will represent a plane, count how many points fits that plane (with some tolerance), repeat this process $ N $ times and take the one that has the maximum number of points. We will give a formula to determine de number $ N $ depending on the probability to be an outlier, the desired probability to find the model and the number of parameters needed to defne it. Once we have the planes, we wil calculate their intersections in order to have significant coners. Then, we will use the angles between the planes to asign a descriptor to the intersection points and determine which of them match in different points clouds.\\

To test the proposed methods, we have different models: a clay feet property of the Faculty of Fine Arts of Granada, a part of \textit{La torre de las Gallinas} in the Alhambra and an office. Theese models have been taken with a Faro Focus 130 laser scanner. It is has also been neccesary to develop an application in which we can code and observe the resuts of the different algorithms. We will cover this part on the last cheapter. This program is written in \textit{C++}, we use \textit{OpenGL} as graphic library and \textit{Qt} for the user interface. During its develepment, we have appliead several concepts related to Computer Graphics. For instance, methods to select or delete a bunch of points or pick one of them. We have also been concerned about the application performance, especially in our case where we use models with lot of vertices.\\

% Keywords command
\providecommand{\keywords}[1]
{
	\small	
	\textbf{\textit{Keywords: }} #1
}
\keywords{theorems of the alternative, Hahn-Banach's theorem, minimax, optimization, financial mathematics, data registration, quaternions, ICP, RANSAC.}